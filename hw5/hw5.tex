\input{../cs446.tex}
\usepackage{amssymb}
\oddsidemargin 0in
\evensidemargin 0in
\textwidth 6.5in
\topmargin -0.5in
\textheight 9.0in

\begin{document}

\solution{Nikhil Unni}{\today}{5}{Fall 2014}


\pagestyle{myheadings}  % Leave this command alone

\begin{enumerate}
\item SVM
  \begin{enumerate}
  \item [(a)]
    \begin{enumerate}  
    \item [1.]
      $\textbf{w} = [-1,0]^T$\\
      $\theta = 0$
    \item [2.]
      $\textbf{w} = [-0.5,0.25]^T$\\
      $\theta = 0$
    \item [3.]
      I found the two closest positive/negative points, $[(-1.2,1.6), +], [(2,0), -]$, and found the slope between them, $\frac{1.6}{-3.2} = -\frac{1}{2}$, and the midpoint, $(0.4, 0.8)$, so the line with the farthest distance between the two points (the support vectors), has a slope of $2$ with a point $(0.4, 0.8)$, giving the line $y=2x$, which gives $w=[-2,1]^T, \theta=0$.\\\\
      Then, I just minimized $w$ by halving it repeatedly, until I got $w=[-0.5,0.25]$. This $w$ gave $y(w^Tx+\theta)=1$ for both support vectors, so I know this is the smallest value of $w$ I can get.      
    \end{enumerate}
  \item [(b)]
    \begin{enumerate}
    \item [1.]
      $I = \left\{1,6\right\}$
    \item [2.]
      $\alpha = \left\{\frac{5}{32},\frac{5}{32}\right\}$
    \item [3.]
      Objective function value = $\frac{5}{32}$.
    \end{enumerate}
  \item [(c)]
    FINISH ME LATER. $C$ represents how much the SVM should avoid misclassifications. In general, $C$ controls the relative importance of maximizing the margin. For $C=\infty$, we obtain our original hyperplane that we found in (a)-2. For $C=1$, we get a larger margin, with a higher chance of misclassification. The support vectors for $C=1$ can now be inside the margins. For $C=0$ has an even wider margin, with even larger misclassification. (FINISH ME LATER)
  \end{enumerate}
\item Kernels
  \begin{enumerate}
    \item[(a)]
      \begin{algorithm}
        1. Initialize $\alpha$ to $\vec{0}$ of length $n$, where $n$ is the number of examples.\\
        2. Initialize $\theta$ to $0$.\\
        3. While still making mistakes (terminate after long string of successes):\\
        4. \>\>For each training example $(x,y)$:\\
        5. \>\>\>if $y[(\sum\limits_{i=1}^n\alpha_i y_i \langle x_i, x \rangle) + \theta] < 0$: ($\langle a,b \rangle$ representing the inner product)\\
        6. \>\>\>\>$\alpha_i \leftarrow \alpha_i + 1$ (where i is the index of the current example $(x,y)$)\\
        7. \>\>\>\>$\theta \leftarrow \theta + y$\\
      \end{algorithm}
    \item[(b)]
      $$K(x,z) = \alpha K_1(x,z) + \beta K_2(x,z)$$
      Since $K_1$ and $K_2$ are both valid kernel functions, they can be represented as the dot product of two feature maps, $\phi_1$ and $\phi_2$ such that
      $$K(x,z) = \alpha \langle \phi_1(x), \phi_1(z) \rangle + \beta \langle \phi_2(x), \phi_2(z) \rangle$$
      And also such that
      $$\phi_1(x) = [\phi_1(x)_1, \phi_1(x)_2, \ldots, \phi_1(x)_M]$$
      $$\phi_2(x) = [\phi_2(x)_1, \phi_2(x)_2, \ldots, \phi_2(x)_N]$$
      Where $M$ and $N$ the size of the vectors $\phi_1(x)$ and $\phi_2(x)$ respectively.
      Then, we can represent $K(x,z)$ by using the definition of $\phi_1$ and $\phi_2$ and expand out the inner products
      $$K(x,z) = \alpha \sum\limits_{i=1}^M \phi_1(x)_i \phi_1(z)_i + \beta \sum\limits_{j=1}^N \phi_2(x)_j \phi_2(z)_j$$
      Then we put $\alpha$ and $\beta$ inside the summations as follows
      $$K(x,z) = \sum\limits_{i=1}^M (\sqrt\alpha \phi_1(x)_i) (\sqrt\alpha \phi_1(z)_i) + \sum\limits_{j=1}^N (\sqrt\beta \phi_2(x)_j) (\sqrt\beta \phi_2(z)_j)$$
      Suppose we had a feature map like :
      $$\phi(x) = [\sqrt\alpha \phi_1(x)_1, \sqrt\alpha \phi_1(x)_2, \ldots, \sqrt\alpha \phi_1(x)_M, \sqrt\beta \phi_2(x)_1, \ldots, \sqrt\beta \phi_2(x)_N]$$
      of dimension $N+M$.\\
      Then, $\langle \phi(x), \phi(z) \rangle = \alpha \phi_1(x)_1 \phi_1(z)_1 + \alpha \phi_1(x)_2 \phi_1(z)_2 + \ldots \alpha \phi_1(x)_M \phi_1(z)_M + \beta \phi_2(x)_1 \phi_2(z)_1 + \ldots + \beta \phi_2(x)_N \phi_2(z)_N$\\
      Or 
      $$\langle \phi(x), \phi(z) \rangle = \sum\limits_{i=1}^M (\sqrt\alpha \phi_1(x)_i) (\sqrt\alpha \phi_1(z)_i) + \sum\limits_{j=1}^N (\sqrt\beta \phi_2(x)_j) (\sqrt\beta \phi_2(z)_j)$$
      $$\langle \phi(x), \phi(z) \rangle = K(x,z)$$
      Because $K(x,z)$ is an inner product of our new feature map, it is a valid kernel for all valid kernels $K_1$ and $K_2$ and all positive $\alpha$ and $\beta$.\\
    \item[(c)]
      Before proving that $K(x,z)$ is a valid kernel, I'll prove one more property of kernels.\\
      \textbf{Note}: I'm going to reuse $K$, $x$ and $z$ for this proof, they're not necessarily the same $K$, $x$ and $z$ we were given in the problem, sorry for the slight abuse of notation!\\
      \begin{enumerate}
        \item[1.] $K(x,z) = K_1(x,z)K_2(x,z)$, for all valid kernels $K_1$ and $K_2$\\
          $$K(x,z) = (\langle \phi_1(x), \phi_1(z) \rangle) * (\langle \phi_2(x), \phi_2(z))$$
          $$K(x,z) = (\sum\limits_{i=1}^M \phi_1(x)_i \phi_1(z)_i)(\sum\limits_{j=1}^N \phi_2(x)_j \phi_2(z)_j)$$
          $$K(x,z) = \sum\limits_{i=1}^M \sum\limits_{j=1}^N \phi_1(x)_i \phi_1(z)_i \phi_2(x)_j \phi_2(z)_j$$
          Let $\phi(x)_{ij} = \phi_1(x)_i * \phi_2(x)_j$ and $\phi(z)_{ij} = \phi_1(z)_i * \phi_2(z)_j$. Then:
          $$K(x,z) = \sum\limits_{i=1}^M \sum\limits_{j=1}^N \phi(x)_{ij} \phi(z)_{ij}$$
          $$K(x,z) = \langle \phi(x), \phi(z) \rangle$$
          Where the dimension of $\phi$ is $M*N$.\\
          Because $K$ is the inner product of the feature map of $x$ and $z$, it's a valid kernel.
      \end{enumerate}
      Now that we have that, we can continue with the original proof.\\\\
      First of all, $K_1(x,z) = x^Tz$ is clearly a valid kernel, where the feature map $\phi_1(x)$ is just the identity feature map, so that $K_1 = \langle \phi_1(x), \phi_1(z) \rangle$.\\\\
      Next, because of my proof right above, $(x^Tz)(x^Tz) = (x^Tz)^2$ is a valid kernel too, since it's just the product of two valid kernels. By the same logic, $(x^Tz)(x^Tz)^2 = (x^Tz)^3$ is also a valid kernel.\\\\
      Then, by the proof from part (b), we know that the linear combination (with positive coefficients) of valid kernels is a valid kernel as well.\\
      So $1(x^Tz)^3 + 400(x^Tz)^2$ is valid, and then $1(x^Tz)^3 + 400(x^Tz)^2 + 100x^Tz$ is a valid kernel as well. $\blacksquare$
      
  \end{enumerate}
\item Answer to problem 3
\end{enumerate}

\end{document}

